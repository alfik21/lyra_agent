#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import json
import sys
import os
import subprocess
from pathlib import Path
from datetime import datetime

import requests
from openai import OpenAI

# ======= IMPORTY MODU≈Å√ìW =======
from modules.app_tools import tool_APP_CONTROL
from modules.audio_tools import tool_AUDIO_DIAG, tool_AUDIO_FIX
from modules.net_tools import tool_NET_INFO, tool_NET_DIAG, tool_NET_FIX
from modules.system_tools import tool_SYSTEM_DIAG, tool_SYSTEM_FIX, tool_AUTO_OPTIMIZE
from modules.app_guard import (
    tool_APP_GUARD,
    tool_APP_GUARD_STOP,
    tool_APP_GUARD_REMOVE,
    tool_APP_GUARD_LIST,
    tool_APP_GUARD_LOGS
)
from modules.tmux_tools import tool_TMUX_SCREEN_DIAG
from modules.brain import query_brain
from modules.system_monitor import get_status, analyze_status
from modules.systeminfo import tool_SYSINFO
from modules.watchdog import tool_WATCHDOG
from modules.voice_input import tool_VOICE_INPUT
from modules.memory_ai import tool_MEMORY_ANALYZE
from modules.desktop_tools import tool_DESKTOP_DIAG, tool_DESKTOP_FIX
from modules.log_analyzer import tool_LOG_ANALYZE
from modules.model_manager import tool_MODEL_MANAGER
from modules.model_router import query_model
from modules.mode_manager import tool_MODE_MANAGER
from modules.status_monitor import tool_STATUS_MONITOR
from modules.switch_model import switch_model
from modules.model_switcher import tool_MODEL_SWITCHER
from modules.opisy_modelow import tool_MODEL_INFO
from modules.model_list import tool_MODEL_LIST
from modules.model_describe import tool_MODEL_DESCRIBE
from modules.disk_tools import tool_DISK_DIAG

PENDING_CONFIRMATION = None

# ===============================
#    MAPA INTENCJI ‚Üí NARZƒòDZI
# ===============================

def detect_intent(user_text: str):
    t = user_text.lower().strip()

    # SYSTEM / DYSKI
    if "sprawd≈∫ dyski" in t or "sprawdz dyski" in t or "dyski" in t:
        return "DISK_DIAG", ""

    # INTERNET
    if "zdiagnozuj internet" in t or "diagnoza internetu" in t:
        return "NET_DIAG", "auto"
    if "napraw internet" in t:
        return "NET_FIX", "auto"

    # D≈πWIƒòK
    if "zdiagnozuj d≈∫wiƒôk" in t or "zdiagnozuj dzwiek" in t:
        return "AUDIO_DIAG", ""
    if "napraw d≈∫wiƒôk" in t or "napraw dzwiek" in t:
        return "AUDIO_FIX", ""

    # MODELE
    if t == "lista modeli" or t == "modele":
        return "MODEL_LIST", ""

    if t.startswith("u≈ºyj modelu") or t.startswith("uzyj modelu") or t.startswith("lyra u≈ºyj"):
        model = t.replace("lyra","").replace("u≈ºyj modelu","").replace("uzyj modelu","").strip()
        return "MODEL_SWITCHER", model

    # M√ìZG TEST
    if "przetestuj po≈ÇƒÖczenie z m√≥zgiem" in t or "test m√≥zgu" in t:
        return "BRAIN_TEST", ""

    return None, None

def disk_diag():
    out = []
    out.append("=== DYSKI I SYSTEM PLIK√ìW ===\n")

    out.append("‚û§ lsblk:")
    out.append(system_tool("lsblk -o NAME,SIZE,TYPE,MOUNTPOINT,FSTYPE"))

    out.append("\n‚û§ df -h:")
    out.append(system_tool("df -h"))

    out.append("\n‚û§ blkid:")
    out.append(system_tool("blkid"))

    return "\n".join(out)

# Dodatkowe funkcje zwiƒÖzane z obs≈ÇugƒÖ modeli AI:

def tool_APP_GUARD(app, system_tool, log):
    """
    Watchdog dla aplikacji (np. Lutris).
    Zwraca prosty SYSTEM, kt√≥ry monitoruje proces.
    """
    if not app:
        return "[APP_GUARD] Nie podano aplikacji."

    # Log informacyjny
    log(f"[APP_GUARD] Monitorowanie aplikacji: {app}", "agent.log")

    # Na razie zwracamy prosty SYSTEM ‚Äì pe≈Çny watchdog mogƒô Ci dopisaƒá p√≥≈∫niej
    return f"SYSTEM: ps -ef | grep {app}"

def tool_SYSTEM_FIX(arg, system_tool, log):
    global PENDING_CONFIRMATION
    #pm = detect_package_manager(system_tool)

    #if not pm:
    #    return "[B≈ÅƒÑD] Nie wykryto mened≈ºera pakiet√≥w (ani dnf, ani apt)."

   # if pm == "dnf":
   #     cmd = "sudo dnf upgrade -y"
   # else:
    #    cmd = "sudo apt update && sudo apt upgrade -y"

    PENDING_CONFIRMATION = cmd_to_run
    return "Czy chcesz wykonaƒá aktualizacjƒô systemu? (tak/nie)"

def scan_models(base_dirs):
    """
    Przeszukuje podane katalogi i ich podkatalogi w poszukiwaniu plik√≥w modeli AI
    (np. z rozszerzeniami .gguf, .ggml, .bin, .pt) i zwraca s≈Çownik
    {nazwa_modelu: pe≈Çna_≈õcie≈ºka}.
    """
    found = {}
    for base in base_dirs:
        # Je≈õli katalog nie istnieje, pomi≈Ñ go
        if not base:
            continue
        if not os.path.exists(base):
            continue
        for root, dirs, files in os.walk(base):
            for fname in files:
                low = fname.lower()
                if low.endswith(('.gguf', '.ggml', '.bin', '.pt')):
                    model_name = Path(fname).stem
                    found[model_name] = os.path.join(root, fname)
    return found

def tool_MODEL_LIST(arg: str, *unused) -> str:
    """
    Odczytuje plik models.json (zgodnie z konfiguracjƒÖ) i zwraca listƒô
    dostƒôpnych modeli w postaci czytelnego tekstu. Je≈õli plik lub lista sƒÖ
    puste, zwraca odpowiedni komunikat.
    """
    path = MODELS_FILE
    if not path or not os.path.exists(path):
        return "Plik models.json nie zosta≈Ç znaleziony."
    try:
        data = json.load(open(path, "r", encoding="utf-8"))
    except Exception:
        return "Nie uda≈Ço siƒô odczytaƒá models.json."
    available = data.get("available", {})
    if not available:
        return "Lista modeli jest pusta."
    out_lines = []
    for i, (mname, mpath) in enumerate(available.items(), start=1):
        out_lines.append(f"{i}. {mname} -> {mpath}")
    return "\n".join(out_lines)

def tool_MODEL_SCAN(arg: str, *unused) -> str:
    """
    Skanuje system plik√≥w w poszukiwaniu plik√≥w modeli AI i aktualizuje
    plik models.json. Argument mo≈ºe zawieraƒá ≈õcie≈ºkƒô katalogu do przeszukania;
    je≈õli jest pusty, u≈ºywa domy≈õlnego katalogu z konfiguracji (klucz
    'models_dir') lub katalogu bie≈ºƒÖcego.
    Zwraca informacjƒô o liczbie znalezionych modeli.
    """
    # Ustal katalogi do skanowania
    search_dirs = []
    if arg:
        search_dirs.append(arg)
    # Je≈õli w konfiguracji podany jest katalog modeli, u≈ºyj go
    cfg_dir = cfg.get("models_dir")
    if cfg_dir:
        search_dirs.append(cfg_dir)
    # Dodaj katalog lokalny z config local_model_path, je≈õli istnieje
    lm_path = cfg.get("local_model_path")
    if lm_path:
        search_dirs.append(str(Path(lm_path).resolve().parent))
    # Je≈õli nadal brak katalog√≥w, przeszukaj katalog bazy programu
    if not search_dirs:
        search_dirs.append(str(BASE_DIR))
    # Wykonaj skan
    found = scan_models(search_dirs)
    if not found:
        return "Nie znaleziono ≈ºadnych modeli AI w podanych katalogach."
    # Wczytaj istniejƒÖce dane
    data = {"available": {}}
    if MODELS_FILE and os.path.exists(MODELS_FILE):
        try:
            data = json.load(open(MODELS_FILE, "r", encoding="utf-8"))
        except Exception:
            data = {"available": {}}
    # Zaktualizuj lub uzupe≈Çnij plik
    if "available" not in data:
        data["available"] = {}
    data["available"].update(found)
    try:
        with open(MODELS_FILE, "w", encoding="utf-8") as f:
            json.dump(data, f, indent=2, ensure_ascii=False)
    except Exception:
        return "B≈ÇƒÖd zapisu do models.json."
    return f"Dodano lub zaktualizowano {len(found)} modeli."

def tool_MODEL_RECOMMEND(arg: str, *unused) -> str:
    """
    Proponuje optymalny model AI do uruchomienia na lokalnym sprzƒôcie na
    podstawie dostƒôpnych modeli oraz og√≥lnych wytycznych dotyczƒÖcych RAM/VRAM.
    Je≈õli nie jeste≈õmy w stanie zebraƒá informacji o sprzƒôcie, zwraca og√≥lne
    wskaz√≥wki.
    Argument mo≈ºe pozostaƒá pusty.
    """
    # Wczytaj listƒô modeli
    path = MODELS_FILE
    if not path or not os.path.exists(path):
        return "Plik models.json nie istnieje, najpierw wykonaj skan modeli."
    try:
        data = json.load(open(path, "r", encoding="utf-8"))
    except Exception:
        return "Nie uda≈Ço siƒô odczytaƒá models.json."
    available = data.get("available", {})
    if not available:
        return "Brak modeli w models.json. Wykonaj skan, aby je dodaƒá."
    # Spr√≥buj odczytaƒá pamiƒôƒá RAM za pomocƒÖ polecenia 'free -m'
    ram_total = None
    try:
        out = system_tool("free -m")
        for line in out.splitlines():
            if line.lower().startswith("mem:"):
                parts = line.split()
                # kolumna 1: total, 2: used, 3: free
                ram_total = int(parts[1])
                break
    except Exception:
        ram_total = None
    # Logika wyboru modelu: preferuj modele z sufiksem Q8_0 lub Q4_0 przy mniejszej pamiƒôci
    preferred = []
    for name in available.keys():
        lname = name.lower()
        if "q8" in lname or "q4" in lname:
            preferred.append(name)
    # Przyk≈Çad minimalnych wymaga≈Ñ: Mistral 7B Q4 ~8GB RAM, Q8 ~12GB RAM;
    # Bielik 11B Q8 ~12GB VRAM, pe≈Çny ~24GB VRAM.
    if ram_total:
        if ram_total < 16 * 1024:  # mniej ni≈º 16 GB RAM
            if preferred:
                return ("Masz mniej ni≈º 16 GB RAM. Zalecane sƒÖ silnie skwantyzowane modele, "
                        f"np. {preferred[0]}, kt√≥re zu≈ºywajƒÖ mniej pamiƒôci.")
            else:
                return ("Masz mniej ni≈º 16 GB RAM i nie znaleziono modeli skwantyzowanych. "
                        "Rozwa≈º pobranie wersji Q4 lub Q8 modelu Bielik lub Mistral.")
        elif ram_total < 32 * 1024:  # 16-32 GB RAM
            if preferred:
                return ("Masz od 16 do 32 GB RAM. Mo≈ºesz uruchomiƒá modele w wersjach Q8. "
                        f"Dostƒôpne modele: {', '.join(preferred)}.")
            else:
                return ("Masz od 16 do 32 GB RAM, ale brak wersji Q8/Q4 w models.json. "
                        "Rozwa≈º pobranie takiej wersji quantized.")
        else:
            # 32 GB RAM lub wiƒôcej
            full_models = [n for n in available if not any(x in n.lower() for x in ["q8", "q4", "q5"])]
            if full_models:
                return ("Masz co najmniej 32 GB RAM. Mo≈ºesz uruchomiƒá pe≈Çne wersje modeli. "
                        f"Dostƒôpne pe≈Çne modele: {', '.join(full_models)}.")
            else:
                return ("Masz co najmniej 32 GB RAM, ale brak pe≈Çnych wersji modeli w models.json. "
                        "Mo≈ºesz u≈ºywaƒá wersji Q8/Q4 lub pobraƒá pe≈ÇnƒÖ wersjƒô z internetu.")
    else:
        # Nie uda≈Ço siƒô okre≈õliƒá RAM ‚Äì podaj og√≥lne wskaz√≥wki
        if preferred:
            return ("Nie mogƒô odczytaƒá ilo≈õci RAM w systemie. Polecam u≈ºyƒá modeli Q8 lub Q4, "
                    f"np. {preferred[0]}, kt√≥re majƒÖ ni≈ºsze wymagania sprzƒôtowe. Je≈õli masz du≈ºo RAM, "
                    "mo≈ºesz spr√≥bowaƒá pe≈Çnej wersji modelu.")
        else:
            return ("Nie mogƒô odczytaƒá ilo≈õci RAM w systemie i brak wersji Q8/Q4 w models.json. "
                    "Rozwa≈º pobranie modelu quantized (Q8, Q4) lub podaj liczbƒô dostƒôpnego RAM, "
                    "aby zaproponowaƒá odpowiedni model.")

# Funkcja detect_intent_local definiowana lokalnie zamiast oddzielnego modu≈Çu.
def detect_intent_local(user_prompt: str):
    """
    Lokalny router intencji u≈ºytkownika (BEZ u≈ºycia LLM).

    Zadanie funkcji:
    - na podstawie s≈Ç√≥w kluczowych rozpoznaƒá, CO u≈ºytkownik chce zrobiƒá
    - zwr√≥ciƒá krotkƒô: (NAZWA_NARZƒòDZIA, ARGUMENT)
    - je≈ºeli nie rozpozna intencji ‚Üí (None, None)

    Dlaczego to wa≈ºne:
    - chroni ‚Äûm√≥zg‚Äù (LLM) przed zalewem niepotrzebnych prompt√≥w
    - zapobiega b≈Çƒôdom typu: Argument list too long (ollama)
    - przyspiesza reakcjƒô agenta
    """

    # Normalizacja wej≈õcia:
    # - lowercase
    # - usuniƒôcie nadmiarowych spacji
    prompt = user_prompt.lower().strip()

    # =========================================================
    # üß† TEST PO≈ÅƒÑCZENIA / M√ìZGU
    # =========================================================
    # R√≥≈ºne warianty, liter√≥wki, jƒôzyk potoczny
    if any(k in prompt for k in [
        "przetestuj po≈ÇƒÖczenie",
        "test po≈ÇƒÖczenia",
        "czy po≈ÇƒÖczenie dzia≈Ça",
        "czy polaczenie dziala",
        "czy po laczenie dziala",
        "czy dzia≈Ça po≈ÇƒÖczenie",
        "czy dziala",
        "czy m√≥zg dzia≈Ça",
        "czy mozg dziala"
    ]):
        return "BRAIN_TEST", ""

    # =========================================================
    # üåê SIEƒÜ / INTERNET
    # =========================================================
    if any(k in prompt for k in [
        "sprawd≈∫ internet",
        "sprawdz internet",
        "zdiagnozuj internet",
        "internet nie dzia≈Ça",
        "problem z internetem",
        "diagnostyka sieci"
    ]):
        return "NET_DIAG", "auto"
        
        
        
        
    # ===============================================
# SYSTEM INFO
# ===============================================
    if any(k in prompt for k in [
        "system info",
        "system-info",
        "sprawd≈∫ system",
        "sprawdz system",
        "informacje o systemie",
        "w jakim systemie",
        "w jakim systemie jeste≈õ",
        "w jakim systemie jestes",
        "jaki to system",
    ]):
        return "SYSINFO", ""

# ===============================================
# SYSTEM DIAG
# ===============================================
    if any(k in prompt for k in [
        "zdiagnozuj system",
        "diagnoza systemu",
        "diagnostyka systemu",
    ]):
        return "SYSTEM_DIAG", ""

# ===============================================
# STATUS / GDZIE JESTES
# ===============================================
    if any(k in prompt for k in [
        "gdzie jeste≈õ",
        "gdzie jestes",
        "status systemu",
        "monitor systemu",
    ]):
        return "STATUS_MONITOR", ""


    # =========================================================
    # üíΩ DYSKI / SYSTEM PLIK√ìW
    # =========================================================
    


    
    if any(k in prompt for k in [
        "sprawd≈∫ dyski",
        "poka≈º dyski",
        "dyski",
        "partycje",
        "system plik√≥w",
        "wolne miejsce",
        "ile mam miejsca"
    ]):
        return "DISK_DIAG", ""

    # =========================================================
    # üßæ PAMIƒòƒÜ / RETROSPEKCJA
    # =========================================================
    if any(k in prompt for k in [
        "co by≈Ço",
        "co bylo",
        "co siƒô popsu≈Ço",
        "co sie popsu≈Ço",
        "ostatnio",
        "dlaczego nie dzia≈Ça≈Ço",
        "co naprawiali≈õmy",
        "jaki by≈Ç problem",
        "jaki byl problem"
    ]):
        return "MEMORY_SUMMARY", ""

    # =========================================================
    # üß† MODELE AI ‚Äì LISTA DOSTƒòPNYCH MODELI
    # =========================================================
    if any(k in prompt for k in [
        "lista modeli",
        "poka≈º modele",
        "poka≈º listƒô modeli",
        "modele lokalne",
        "lokalne modele",
        "lista lokalnych modeli"
    ]):
        return "MODEL_LIST", ""

    # =========================================================
    # üîç MODELE AI ‚Äì SKANOWANIE / OD≈öWIE≈ªANIE
    # =========================================================
    if any(k in prompt for k in [
        "szukaj modeli",
        "poszukaj modeli",
        "od≈õwie≈º listƒô modeli",
        "przeszukaj dysk",
        "przeszukaj komputer",
        "przeskanuj modele",
        "skanuj modele"
    ]):
        return "MODEL_SCAN", ""

    # =========================================================
    # ü§ñ MODELE AI ‚Äì REKOMENDACJA / WYB√ìR
    # =========================================================
    if any(k in prompt for k in [
        "optymalny model",
        "jaki model",
        "kt√≥ry model",
        "lepszy model",
        "jaki model wybraƒá",
        "model do mojego komputera"
    ]):
        return "MODEL_RECOMMEND", ""

    # =========================================================
    # üîä D≈πWIƒòK / AUDIO
    # =========================================================
    if any(k in prompt for k in [
        "zdiagnozuj d≈∫wiƒôk",
        "diagnostyka d≈∫wiƒôku",
        "problemy z d≈∫wiƒôkiem",
        "brak d≈∫wiƒôku"
    ]):
        return "AUDIO_DIAG", "auto"

    # =========================================================
    # üñ•Ô∏è ≈öRODOWISKO GRAFICZNE ‚Äì CINNAMON
    # =========================================================
    if "cinnamon" in prompt:
        # Je≈ºeli u≈ºytkownik wyra≈∫nie chce naprawy
        if any(k in prompt for k in ["napraw", "fix", "naprawa"]):
            return "DESKTOP_FIX", "cinnamon"
        # Domy≈õlnie: diagnostyka
        return "DESKTOP_DIAG", "cinnamon"

    # =========================================================
    # üìÑ ANALIZA LOG√ìW
    # =========================================================
    # Akceptuje polecenia typu:
    # - "analizuj log ~/.xsession-errors"
    # - "analiza log /var/log/syslog"
    if "analizuj log" in prompt or "analiza log" in prompt:
        parts = user_prompt.split("log", 1)
        arg_path = parts[1].strip() if len(parts) > 1 else ""
        return "LOG_ANALYZE", arg_path
        
    # ============================================
    # ===========================
    #   APLIKACJE ‚Äì WATCHDOG
    # ===========================
    if any(k in prompt for k in [
        "pilnuj", "watchdog", "monitoruj", "nadzoruj", "pilnowanie",
        "strze≈º", "kontroluj aplikacjƒô", "pilnuj lutris"
    ]):
        # wyciƒÖgamy nazwƒô aplikacji po s≈Çowie ‚Äûpilnuj‚Äù
        parts = prompt.split()
        try:
            app = parts[parts.index("pilnuj") + 1]
        except:
            app = "auto"
        return "APP_GUARD", app
 

    if any(k in prompt for k in [
        "updatuj system",
        "update systemu",
        "zaktualizuj system",
        "wykonaj aktualizacje systemu",
        "aktualizacja systemu",
        "aktualizacje"
    ]):
        return "SYSTEM_FIX", "update"

    if prompt.startswith("pilnuj "):
        return "APP_GUARD", prompt.replace("pilnuj ", "")

    if prompt.startswith("zatrzymaj watchdog "):
        return "APP_GUARD_STOP", prompt.replace("zatrzymaj watchdog ", "")

    if prompt.startswith("usu≈Ñ watchdog "):
        return "APP_GUARD_REMOVE", prompt.replace("usu≈Ñ watchdog ", "")

    if "pilnowane" in prompt:
        return "APP_GUARD_LIST", ""

    if prompt.startswith("logi "):
        return "APP_GUARD_LOGS", prompt.replace("logi ", "")
    # =========================================================
    # ‚ùì BRAK ROZPOZNANEJ INTENCJI
    # =========================================================
    return None, None


# ======= ≈öCIE≈ªKI I KONFIGURACJA =======
BASE_DIR = Path(__file__).resolve().parent
CFG_PATH = BASE_DIR / "config.json"
PAMIEC_ROBOCZA = (BASE_DIR / "agent_memory.json").resolve()
PAMIEC_DLUGA = (BASE_DIR / "agent_memory_long.json").resolve()
PAMIEC_ARCHIWUM = (BASE_DIR / "agent_memory_archive.json").resolve()

for sciezka, domyslna in [
    (PAMIEC_ROBOCZA, []),
    (PAMIEC_DLUGA, {}),
    (PAMIEC_ARCHIWUM, [])
]:
    if not sciezka.exists():
        sciezka.write_text(
            json.dumps(domyslna, indent=2, ensure_ascii=False),
            encoding="utf-8"
        )


# Upewnij siƒô, ≈ºe plik konfiguracyjny istnieje.
if not CFG_PATH.exists():
    raise SystemExit(f"[Lyra] Brak {CFG_PATH}. Utw√≥rz config.json.")

# Wczytaj konfiguracjƒô i upewnij siƒô, ≈ºe wymagane klucze majƒÖ warto≈õci domy≈õlne.
cfg = json.loads(CFG_PATH.read_text(encoding="utf-8"))
default_keys = {
    "api_key": "key"
    "user_name": "Tomek",
    "temperature": 0.4,
    "backend": "local",
    "local_model": "aya-23-8B-f16",
    "openai_model": "gpt-5.1",
    "ollama_model": "mistral",
    "n_ctx": 4096,
    "n_batch": 512,
    "mlock": False,
    "tools_enabled": "true",
    "threads": 12,
    "model": "aya-23-8B-f16",
    
    "local_model_path": "/media/tomek/arhiwum/AI_MODELS/aya-23-8B-f16.gguf",
    "memory_file": "agent_memory.json",
    "state_file": "agent_state.json",
    "models_file": "models.json",
    "logs_dir": "logs",
    # cfg.get("logs_dir", "logs")
    }
#for k, v in default_keys.items():
 #   if k not in cfg:
  #      cfg[k] = v
#FILTR ≈öMIECI (BARDZO WA≈ªNE)

# =========================
# FILTR OCHRONNY (≈öMIECI / LOGI / B≈ÅƒòDY)
# =========================

def czy_warto_zapamietac(tekst: str) -> bool:
    if not tekst:
        return False

    zakazane = [
        "Argument list too long",
        "timed out",
        "Traceback",
        "[Errno",
        "HTTPConnectionPool",
        "Read timed out",
        "ollama",
        "ERROR",
        "Exception"
    ]
    return not any(z in tekst for z in zakazane)

# =========================
# LIMITY BEZPIECZE≈ÉSTWA
# =========================

MAX_KONTEKST = 5
MAX_DLUGOSC_TEKSTU = 1200
MAX_DLUGOSC_PROMPTU = 3500

# =========================
# PAMIƒòƒÜ ROBOCZA (KR√ìTKA)
# =========================

def wczytaj_pamiec():
    return json.loads(PAMIEC_ROBOCZA.read_text(encoding="utf-8"))

def zapisz_pamiec(wpis: dict):
    pamiec = wczytaj_pamiec()

    tresc = wpis.get("assistant") or wpis.get("output") or ""
    if tresc and not czy_warto_zapamietac(tresc):
        return

    if wpis.get("assistant") and len(wpis["assistant"]) > MAX_DLUGOSC_TEKSTU:
        wpis["assistant"] = wpis["assistant"][:MAX_DLUGOSC_TEKSTU] + " ‚Ä¶[uciƒôto]"

    pamiec.append(wpis)

    PAMIEC_ROBOCZA.write_text(
        json.dumps(pamiec, indent=2, ensure_ascii=False),
        encoding="utf-8"
    )

    kompresuj_pamiec_jesli_trzeba()

# =========================
# KOMPRESJA ‚Üí ARCHIWUM (NIC NIE GINIE)
# =========================

def kompresuj_pamiec_jesli_trzeba():
    pamiec = wczytaj_pamiec()
    if len(pamiec) < 25:
        return

    archiwum = json.loads(PAMIEC_ARCHIWUM.read_text(encoding="utf-8"))

    stare = pamiec[:-10]
    archiwum.extend(stare)

    podsumowanie = {
        "time": datetime.now().isoformat(),
        "type": "PODSUMOWANIE",
        "content": "Starsza rozmowa przeniesiona do archiwum."
    }

    PAMIEC_ARCHIWUM.write_text(
        json.dumps(archiwum, indent=2, ensure_ascii=False),
        encoding="utf-8"
    )

    PAMIEC_ROBOCZA.write_text(
        json.dumps(pamiec[-10:] + [podsumowanie], indent=2, ensure_ascii=False),
        encoding="utf-8"
    )

# =========================
# PAMIƒòƒÜ D≈ÅUGOTERMINOWA (NIE≈öMIERTELNA)
# =========================

def zapamietaj_na_zawsze(klucz: str, wartosc):
    dane = json.loads(PAMIEC_DLUGA.read_text(encoding="utf-8"))
    dane[klucz] = wartosc

    PAMIEC_DLUGA.write_text(
        json.dumps(dane, indent=2, ensure_ascii=False),
        encoding="utf-8"
    )

# =========================
# BUDOWA KONTEKSTU DO M√ìZGU
# =========================

def pobierz_kontekst_do_promptu():
    pamiec = wczytaj_pamiec()
    return [
        m for m in pamiec
        if m.get("type") == "TEXT"
    ][-MAX_KONTEKST:]

# =========================
# OCHRONA M√ìZGU (PROMPT)
# =========================

def zabezpiecz_prompt(prompt: str) -> str:
    if len(prompt) > MAX_DLUGOSC_PROMPTU:
        log("PROMPT ZA D≈ÅUGI ‚Üí SKRACAM", "agent.log")
        return prompt[-MAX_DLUGOSC_PROMPTU:]
    return prompt


## =========================
# TEST M√ìZGU (FUNKCJA POMOCNICZA ‚Äì BEZPIECZNA)
# =========================

def test_mozgu(cfg):
    """
    Test po≈ÇƒÖczenia z lokalnym m√≥zgiem.
    Zawsze zwraca aktualnie za≈Çadowany model.
    """
    model = (
        cfg.get("model")
        or cfg.get("local_model")
        or cfg.get("ollama_model")
        or "nieznany"
    )
    out = query_brain("Odpowiedz kr√≥tko: czy dzia≈Çasz?")
    return f"My≈õlenie lokalne dzia≈Ça, Tomek. Aktywny model: {model}.\n\n{out}"
    local_model = cfg.get("local_model")
    
def polish_guard(text: str) -> str:
    return (
        "ODPOWIADAJ WY≈ÅƒÑCZNIE PO POLSKU.\n"
        "NIE U≈ªYWAJ JƒòZYKA ANGIELSKIEGO.\n"
        "BƒÑD≈π TECHNICZNY I KONKRETNY.\n\n"
        + text
    )
    

#lyra przetestuj po≈ÇƒÖczenie z m√≥zgiem
#lyra zdiagnozuj d≈∫wiƒôk
#lyra sprawd≈∫ dyski
#lyra zdiagnozuj cinnamon



# Zapisz zaktualizowanƒÖ konfiguracjƒô na dysk, aby nowe klucze siƒô zachowa≈Çy.
CFG_PATH.write_text(json.dumps(cfg, indent=2, ensure_ascii=False), encoding="utf-8")

# Ustaw domy≈õlnƒÖ nazwƒô u≈ºytkownika, je≈õli brak.
if "user_name" not in cfg:
    cfg["user_name"] = "Tomek"
    CFG_PATH.write_text(json.dumps(cfg, indent=2, ensure_ascii=False), encoding="utf-8")

# ======= LOGOWANIE =======
# Zdefiniuj katalog log√≥w i upewnij siƒô, ≈ºe istnieje.
LOGS_DIR = BASE_DIR / cfg.get("logs_dir", "logs")
LOGS_DIR.mkdir(parents=True, exist_ok=True)

def log(line: str, filename: str = "agent.log"):
    ts = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    with (LOGS_DIR / filename).open("a", encoding="utf-8") as f:
        f.write(f"[{ts}] {line}\n")

# ======= ZARZƒÑDZANIE PAMIƒòCIƒÑ =======
# ≈öcie≈ºki do plik√≥w pamiƒôci i stanu na podstawie konfiguracji.
MEMORY_PATH = (BASE_DIR / cfg.get("memory_file", "agent_memory.json")).resolve()
STATE_PATH = (BASE_DIR / cfg.get("state_file", "agent_state.json")).resolve()

# Upewnij siƒô, ≈ºe pliki pamiƒôci i stanu istniejƒÖ.
if not MEMORY_PATH.exists():
    MEMORY_PATH.write_text("[]", encoding="utf-8")
if not STATE_PATH.exists():
    STATE_PATH.write_text("{}", encoding="utf-8")

def load_memory():
    """Wczytaj historiƒô rozmowy z dysku."""
    try:
        return json.loads(MEMORY_PATH.read_text(encoding="utf-8"))
    except Exception:
        return []

def save_memory(entry: dict):
    """Dodaj wpis pamiƒôci do historii rozmowy."""
    mem = load_memory()
    mem.append(entry)
    MEMORY_PATH.write_text(json.dumps(mem, indent=2, ensure_ascii=False), encoding="utf-8")

def remember(user: str, assistant: str):
    """Zapamiƒôtaj wymianƒô tekstowƒÖ u≈ºytkownik/asystent."""
    save_memory({
        "time": datetime.now().isoformat(),
        "type": "TEXT",
        "user": user,
        "assistant": assistant
    })

def remember_entry(entry_type: str, **fields):
    """Zapamiƒôtaj zdarzenie inne ni≈º tekstowe, np. u≈ºycie narzƒôdzia lub wykonanie systemowe."""
    entry = {"time": datetime.now().isoformat(), "type": entry_type}
    entry.update(fields)
    save_memory(entry)

# ======= STAN PRACY (STATE) =======
def load_state() -> dict:
    """Wczytaj zapisany stan agenta z dysku."""
    try:
        return json.loads(STATE_PATH.read_text(encoding="utf-8"))
    except Exception:
        return {}

def save_state(state: dict):
    """Zapisz stan agenta na dysku."""
    STATE_PATH.write_text(json.dumps(state, indent=2, ensure_ascii=False), encoding="utf-8")

def update_state(patch: dict):
    """Uaktualnij zapamiƒôtany stan czƒô≈õciowƒÖ poprawkƒÖ."""
    st = load_state()
    st.update(patch)
    save_state(st)

# ======= NARZƒòDZIA SYSTEMOWE =======
def system_tool(cmd: str, timeout: int = 15) -> str:
    """Wykonaj polecenie pow≈Çoki, opcjonalnie proszƒÖc o potwierdzenie sudo."""
    log(f"SYSTEM CMD: {cmd}", "system.log")
    # optional guard for sudo
    if "sudo" in cmd:
        print("‚ö†Ô∏è Polecenie wymaga sudo. Potwierd≈∫: TAK")
        if input("> ").strip().lower() != "tak":
            return "‚ùå Anulowano przez u≈ºytkownika."
    try:
        return subprocess.check_output(
            cmd, shell=True,
            stderr=subprocess.STDOUT,
            timeout=timeout, text=True
        )
    except subprocess.TimeoutExpired:
        return f"[TIMEOUT] {cmd}"
    except subprocess.CalledProcessError as e:
        return e.output or "[ERROR]"
        
        
def handle_confirmation(user_prompt: str):
    global PENDING_CONFIRMATION
    if not PENDING_CONFIRMATION:
        return None

    p = user_prompt.lower().strip()

    if p in ["tak", "y", "yes", "potwierdzam", "lyra tak", "lyra potwierdzam"]:
        cmd = PENDING_CONFIRMATION
        PENDING_CONFIRMATION = None
        print(f"SYSTEM: {cmd}")
        return system_tool(cmd)

    if p in ["nie", "n", "no", "anuluj"]:
        print("‚ùé Anulowano.")
        PENDING_CONFIRMATION = None
        return ""

    print("‚ùó Oczekujƒô potwierdzenia (tak/nie).")
    return ""

def detect_package_manager(system_tool):
    """
    Automatyczna detekcja mened≈ºera pakiet√≥w:
    - dnf (Fedora / Nobara / RHEL)
    - apt (Ubuntu / Mint / Debian)
    """
    if system_tool("command -v dnf").strip():
        return "dnf"
    if system_tool("command -v apt").strip():
        return "apt"
    return None

def tmux_capture(lines: int = 500) -> str:
    """Przechwyƒá dane z panelu tmux."""
    cmd = f"tmux capture-pane -pS -{lines}"
    return system_tool(cmd, timeout=3)

# ======= WYB√ìR MODELU =======
MODELS_FILE = cfg.get("models_file")

def pick_local_model(cfg: dict):
    """Wybierz model lokalny z models.json na podstawie konfiguracji."""
    wanted = (cfg.get("local_model") or "").lower()
    if not MODELS_FILE or not os.path.exists(MODELS_FILE):
        return "[Lyra-System] Brak odpowiedzi z narzƒôdzia."
    data = json.load(open(MODELS_FILE, "r", encoding="utf-8"))
    for name, path in data.get("available", {}).items():
        if wanted and wanted in name.lower():
            # Zapisz ≈õcie≈ºkƒô w konfiguracji do p√≥≈∫niejszego u≈ºycia
            cfg["local_model_path"] = path
            return name, path
    return None, None

# Okre≈õl, kt√≥rego modelu u≈ºyƒá przy starcie: lokalnego czy OpenAI.
local_name, local_path = pick_local_model(cfg)
if cfg.get("backend") == "local" and local_path:
    cfg["model"] = local_name
    cfg["local_model_path"] = local_path
else:
    cfg["backend"] = "openai"

def pick_openai_model(cfg: dict) -> str:
    """Zwr√≥ƒá nazwƒô modelu OpenAI z konfiguracji lub domy≈õlnƒÖ."""
    return cfg.get("openai_model") or cfg.get("model") or "gpt-5.1"

# Zainicjuj klienta OpenAI, je≈õli podano klucz API.
client = OpenAI(api_key=cfg.get("api_key", "")) if cfg.get("api_key") else None

# ======= URUCHAMIANIE NARZƒòDZI =======
# ============================
#   NARZƒòDZIA LYRY ‚Äì DISPATCH
# ============================
def tool_dispatch(name: str, arg: str) -> str:
    """
    Centralny router narzƒôdzi. Ka≈ºda komenda Lyry po wykryciu intencji
    trafia tutaj, a dalej jest kierowana do odpowiedniego modu≈Çu.
    """
    name = (name or "").strip().upper()

    # --- test m√≥zgu ---
    if name == "BRAIN_TEST":
        return test_mozgu(cfg)

    # --- pamiƒôƒá tekstowa ---
    if name == "MEMORY_SUMMARY":
        mem = load_memory()
        if not mem:
            return "Brak zapisanej historii."
        last = mem[-20:]
        out = ["Ostatnie wpisy pamiƒôci:"]
        for m in last:
            u = m.get("user", "").strip()
            a = m.get("assistant", "").strip()
            if u or a:
                out.append(f"- {u} ‚Üí {a[:120]}")
        return "\n".join(out)

    # =========================
    #        SIEƒÜ
    # =========================
    if name == "NET_INFO":
        return tool_NET_INFO(arg, system_tool, log)

    if name == "NET_DIAG":
        return tool_NET_DIAG(arg, system_tool, log)

    if name == "NET_FIX":
        return tool_NET_FIX(arg, system_tool, log)

    # =========================
    #        D≈πWIƒòK
    # =========================
    if name == "AUDIO_DIAG":
        return tool_AUDIO_DIAG(arg, system_tool, log)

    if name == "AUDIO_FIX":
        return tool_AUDIO_FIX(arg, system_tool, log)

    # =========================
    #        SYSTEM
    # =========================
    if name == "SYSTEM_DIAG":
        return tool_SYSTEM_DIAG(arg, system_tool, log)

    if name == "SYSTEM_FIX":
        return tool_SYSTEM_FIX(arg, system_tool, log)

    if name == "AUTO_OPTIMIZE":
        return tool_AUTO_OPTIMIZE(arg, system_tool, log)

    if name == "SYSINFO":
        return tool_SYSINFO(arg, system_tool, log)

    # =========================
    #        DESKTOP / GUI
    # =========================
    if name in ["CINNAMON_DIAG", "KDE_DIAG", "GNOME_DIAG", "XFCE_DIAG", "DESKTOP_DIAG"]:
        return tool_DESKTOP_DIAG(arg, system_tool, log)

    if name in ["CINNAMON_FIX", "KDE_FIX", "GNOME_FIX", "XFCE_FIX", "DESKTOP_FIX"]:
        return tool_DESKTOP_FIX(arg, system_tool, log)

    # =========================
    #        TMUX
    # =========================
    if name == "TMUX_SCREEN_DIAG":
        return tool_TMUX_SCREEN_DIAG(arg, system_tool, log)

    # =========================
    #       WATCHDOG / STATUS
    # =========================
    if name == "STATUS_MONITOR":
        return tool_STATUS_MONITOR(arg, system_tool, log)

    if name == "WATCHDOG":
        return tool_WATCHDOG(arg, system_tool, log)

    # =========================
    #          LOGI
    # =========================
    if name == "LOG_ANALYZE":
        return tool_LOG_ANALYZE(arg, system_tool, log)

    # =========================
    #        PAMIƒòƒÜ AI
    # =========================
    if name == "MEMORY_ANALYZE":
        return tool_MEMORY_ANALYZE(arg, system_tool, log)

    # =========================
    #        MODELE
    # =========================
    if name == "MODEL_LIST":
        return tool_MODEL_LIST(arg, system_tool, log)

    if name == "MODEL_MANAGER":
        return tool_MODEL_MANAGER(arg, system_tool, log)

    if name in ["MODEL_SWITCHER", "PRZE≈ÅƒÑCZNIK_MODELI"]:
        return switch_model(arg, log)

    if name == "MODEL_DESCRIBE":
        return tool_MODEL_DESCRIBE(arg, system_tool, log)

    if name == "MODEL_INFO":
        return tool_MODEL_INFO(arg, system_tool, log)
    
        # =========================
    # MODELE ‚Äì SKANOWANIE / INFO / REKOMENDACJA
    # =========================

    if name == "MODEL_SCAN":
        return tool_MODEL_SCAN(arg)

   # if name == "MODEL_LIST":
    #    return tool_MODEL_LIST(arg)

    if name == "MODEL_RECOMMEND":
        return tool_MODEL_RECOMMEND(arg)

    
    # =========================
    #        APLIKACJE
    # =========================
    if name == "APP_CONTROL":
        return tool_APP_CONTROL(arg, system_tool, log)

    #if name == "APP_GUARD":
     #   return tool_APP_GUARD(arg, system_tool, log)

    # =========================
    #        DYSKI
    # =========================
    if name == "DISK_DIAG":
        return disk_diag()

    # =========================
    #    WEJ≈öCIE G≈ÅOSOWE
    # =========================
    if name == "VOICE_INPUT":
        return tool_VOICE_INPUT(arg, system_tool, log)


    # ==========================
    #   WATCHDOG / APLIKACJE
    # ==========================
    if name == "APP_GUARD":
        return tool_APP_GUARD(arg, system_tool, log)

    # ==========================
    #   SYSTEM FIX / UPDATE
    # ==========================
    #if name == "SYSTEM_FIX":
     #   return tool_SYSTEM_FIX(arg)


    if name == "APP_GUARD": return tool_APP_GUARD(arg)
    if name == "APP_GUARD_STOP": return tool_APP_GUARD_STOP(arg)
    if name == "APP_GUARD_REMOVE": return tool_APP_GUARD_REMOVE(arg)
    if name == "APP_GUARD_LIST": return tool_APP_GUARD_LIST(arg)
    if name == "APP_GUARD_LOGS": return tool_APP_GUARD_LOGS(arg)

    # =========================
    #     BRAK NARZƒòDZI
    # =========================
    return f"[Lyra] Nie znam narzƒôdzia: {name}"


# ======= BANER INFORMACYJNY =======
def print_lyra_banner(cfg):
    """
    Drukuje baner informacyjny podsumowujƒÖcy wybrany backend, model oraz dostƒôpno≈õƒá internetu.
    """
    mode = "LOCAL" if cfg.get("backend") == "local" else "ONLINE"
    model = cfg.get("model") if mode == "LOCAL" else pick_openai_model(cfg)
    net = "‚úÖ jest" if cfg.get("internet", True) else "‚ùå brak"
    print(f"[Lyra] ≈ªƒÖdany model lokalny: {cfg.get('local_model', '')}")
    if mode == "LOCAL":
        print(f"[Lyra MODEL] Wybrano model lokalny: {cfg.get('model')}")
        print(f"[Lyra MODEL] ≈öcie≈ºka: {cfg.get('local_model_path')}")
    else:
        print(f"[Lyra MODEL] Backend zdalny (OpenAI)")
    print(f"[Lyra ‚Ä¢ Tryb: {mode} ‚Ä¢ Model: {model} ‚Ä¢ Internet: {net}]")
    print(f"Data: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")

# ======= G≈Å√ìWNA PƒòTLA WYKONAWCZA =======
# ======= G≈Å√ìWNA PƒòTLA WYKONAWCZA =======
def run_once(user_prompt: str):
    # ============================================
    # SYSTEM POTWIERDZANIA OPERACJI
    # ============================================
    result = handle_confirmation(user_prompt)
    if result is not None:
        return result

# ============================================
# SYSTEM POTWIERDZANIA POLECE≈É
# ============================================
def handle_confirmation(user_prompt: str):
    global PENDING_CONFIRMATION

    if not PENDING_CONFIRMATION:
        return None  # nic nie potwierdzamy

    normalized = user_prompt.lower().strip()

    # POTWIERDZENIE
    if normalized in ["tak", "yes", "y", "lyra tak", "potwierdzam", "lyra potwierdzam"]:
        cmd = PENDING_CONFIRMATION
        PENDING_CONFIRMATION = None
        print(f"SYSTEM: {cmd}")
        return system_tool(cmd)

    # ANULOWANIE
    if normalized in ["nie", "no", "n", "lyra nie", "anuluj", "lyra anuluj"]:
        print("‚ùé Operacja anulowana.")
        PENDING_CONFIRMATION = None
        return ""

    # Inne odpowiedzi ‚Üí dalej czekamy
    print("‚ùó Oczekujƒô na potwierdzenie (tak/nie).")
    return ""

    # ======= TU DOPIERO RESZTA KODU run_once =======
    # ===============================
    #  INTENT ROUTER ‚Äî wykrywanie narzƒôdzi
    # ===============================
   # intent, arg = detect_intent(user_prompt)
    
   # if intent:
    
    #    return tool_dispatch(intent, arg)
    # je≈õli brak narzƒôdzia ‚Üí odpowied≈∫ LLM
    #return answer_from_llm    
    """
    Obs≈Çuguje pojedyncze zapytanie u≈ºytkownika.

    Zasada dzia≈Çania (kolejno≈õƒá jest bardzo wa≈ºna):
    0) normalizacja wej≈õcia
    1) wykrycie intencji lokalnej (bez LLM) -> tool_name/arg
    2) szybkie komendy i skr√≥ty (BRAIN_TEST, zmiana modelu, SYSTEM:, TOOL:)
    3) narzƒôdzia wykryte lokalnie
    4) filtry bezpiecze≈Ñstwa (kr√≥tkie i czytelne komunikaty)
    5) budowa kontekstu/pamiƒôci
    6) wywo≈Çanie modelu (local/openai)
    7) fallback (m√≥zg) je≈õli LLM padnie lub zwr√≥ci pustkƒô
    """

    # ---------------------------------------------------------
    # 0) NORMALIZACJA WEJ≈öCIA
    # ---------------------------------------------------------
    prompt_lower = (user_prompt or "").lower().strip()

    # ---------------------------------------------------------
    # 1) INTENCJE LOKALNE ‚Äì MUSZƒÑ BYƒÜ WCZE≈öNIE (≈ºeby nie by≈Ço UnboundLocalError)
    # ---------------------------------------------------------
   # global PENDING_CONFIRMATION

# JE≈öLI CZEKA NA POTWIERDZENIE
    #if PENDING_CONFIRMATION:
     #   if user_prompt.lower() in ("tak", "y", "yes"):
      #      cmd = PENDING_CONFIRMATION
      #      PENDING_CONFIRMATION = None
      #      print(f"SYSTEM: {cmd}")
      #      return
      #  elif user_prompt.lower() in ("nie", "n", "no"):
      #      print("‚ùé Anulowano wykonanie polecenia.")
      #      PENDING_CONFIRMATION = None
      #      return
      #  else:
      #      print("‚ö†Ô∏è Oczekujƒô odpowiedzi: tak / nie.")
      #      return
    # Dziƒôki temu tool_name istnieje ZAWSZE, zanim cokolwiek go u≈ºyje.
    # tool_name, arg = detect_intent_local(user_prompt) or (None, "")
    tool_name, arg = detect_intent_local(user_prompt)
    # ---------------------------------------------------------
    # 2) TEST M√ìZGU: OBS≈ÅUGA LOKALNA (BEZ LLM / BEZ OLLAMA)
    # ---------------------------------------------------------
    if tool_name:
        out = tool_dispatch(tool_name, arg or "")
        print(out)
        remember_entry(
            "TOOL",
            user=user_prompt,
            tool=tool_name,
            args=arg,
            output=str(out)[:4000]
        )
        return
    # To jest krytyczne: test po≈ÇƒÖczenia nie mo≈ºe przepychaƒá ogromnego promptu do LLM.
    if tool_name == "BRAIN_TEST":
        model = cfg.get("local_model", cfg.get("model", "nieznany"))
        msg = (
            f"My≈õlenie lokalne dzia≈Ça, Tomek.\n"
            f"Aktywny model: {model}.\n\n"
            "M√≥zg jest aktywny i reaguje lokalnie."
        )
        print(msg)
        remember(user_prompt, msg)
        return

    # ---------------------------------------------------------
    # 3) ‚ÄûPRZETESTUJ PO≈ÅƒÑCZENIE Z M√ìZGIEM‚Äù ‚Äì stary skr√≥t zostaje (nie usuwam)
    # ---------------------------------------------------------
    # Pozostawiamy, bo Ty tego u≈ºywasz, a to jest czytelne.
    # To NIE wywo≈Ça LLM, bo tool_dispatch(BRAIN_TEST) ma byƒá lokalne.
    if "przetestuj po≈ÇƒÖczenie z m√≥zgiem" in prompt_lower:
        out = tool_dispatch("BRAIN_TEST", "")
        print(out)
        remember(user_prompt, out)
        return

    # --- NATYCHMIASTOWY TEST M√ìZGU (BEZ MODELU) ---
    if any(k in prompt_lower for k in [
        "czy po≈ÇƒÖczenie dzia≈Ça",
        "czy polaczenie dziala",
        "czy po laczenie dziala",
        "czy dzia≈Ça po≈ÇƒÖczenie",
        "czy m√≥zg dzia≈Ça",
        "przetestuj po≈ÇƒÖczenie"
    ]):
        model = cfg.get("local_model", cfg.get("model", "nieznany"))
        msg = (
            "‚úÖ PO≈ÅƒÑCZENIE DZIA≈ÅA\n\n"
            f"Aktywny model lokalny: {model}\n"
            "Silnik lokalny odpowiada poprawnie.\n"
            "Nie u≈ºywam modelu zewnƒôtrznego."
        )
        print(msg)
        remember(user_prompt, msg)
        return



    # ---------------------------------------------------------
    # 4) WYB√ìR MODELU LOKALNEGO / MAPOWANIE NA NAZWY ‚ÄûSILNIKA‚Äù
    # ---------------------------------------------------------
    # Ten blok zostaje, ale dodajƒô komentarze i minimalnƒÖ ochronƒô.
    # Uwaga: To mapuje cfg["model"] na nazwy, kt√≥rych u≈ºywa backend (np. ollama).
    local_model = (cfg.get("local_model") or "").lower()

    if local_model == "mistral":
        cfg["model"] = "mistral:latest"
    elif local_model == "bielik":
        cfg["model"] = "bielik-11b"
    elif local_model == "llama":
        cfg["model"] = "llama3.1:latest"
    elif local_model == "mixtral":
        cfg["model"] = "mixtral:8x7b"
    elif local_model == "gemma":
        # Obs≈Çuga modelu Gemma ‚Äì u≈ºyj nazwy bazowej lub suffiksu latest
        cfg["model"] = "gemma:latest"
    elif local_model == "aya":
        # Dla modelu AYA u≈ºyj nazwy z konfiguracji lub domy≈õlnej
        cfg["model"] = cfg.get("model", "aya-23-8B-f16")

    # ---------------------------------------------------------
    # 5) ZMIANA MODELU ‚Äì masz DWIE ≈öCIE≈ªKI, nie usuwam ≈ºadnej
    # ---------------------------------------------------------
    # (A) stara: ‚Äûlyra zmie≈Ñ model na ‚Ä¶‚Äù
    if prompt_lower.startswith("lyra zmie≈Ñ model na"):
        name = user_prompt.split("na", 1)[1].strip()
        cfg["local_model"] = name
        cfg["model"] = name
        cfg["backend"] = "local"
        with open(CFG_PATH, "w", encoding="utf-8") as f:
            json.dump(cfg, f, indent=2, ensure_ascii=False)
        print(f"‚úÖ Zmieniono model lokalny na: {name}")
        print_lyra_banner(cfg)
        return

    # (B) nowsza: ≈Çapie te≈º ‚Äûzmie≈Ñ model ‚Ä¶‚Äù
    # Zostaje, ale zmieniam sys.exit(0) -> return, ≈ºeby nie ubijaƒá procesu w ≈õrodku (bezpieczniej).
    if "zmie≈Ñ model" in prompt_lower:
        try:
            new_model = user_prompt.split()[-1].lower()
        except Exception:
            new_model = ""
        if new_model:
            # state = load_state()  # zostawiam (nie usuwam), ale nie musimy go tu u≈ºywaƒá
            state = load_state()
            cfg["local_model"] = new_model
            with open(BASE_DIR / "config.json", "w", encoding="utf-8") as f:
                json.dump(cfg, f, indent=2, ensure_ascii=False)
            print(f"‚úÖ Zmieniono model na: {new_model}")
            # sys.exit(0)  # NIE USUWAM W SENSIE LOGICZNYM ‚Äì tylko zastƒôpujƒô bezpiecznym return
            return

    # ---------------------------------------------------------
    # 6) JAWNE POLECENIE SYSTEMOWE
    # ---------------------------------------------------------
    if user_prompt.lower().startswith("system:"):
        cmd = user_prompt.split(":", 1)[1].strip()
        out = system_tool(cmd)
        print(out)
        update_state({"last_system_cmd": cmd, "last_system_output": out[:4000]})
        remember_entry("SYSTEM", user=user_prompt, command=cmd, output=out[:4000])
        return

    # ---------------------------------------------------------
    # 7) JAWNE U≈ªYCIE NARZƒòDZIA TOOL:
    # ---------------------------------------------------------
    if user_prompt.startswith("TOOL:"):
        rest = user_prompt.split(":", 1)[1].strip()
        _tool_name, _arg = (rest.split("|", 1) + [""])[:2]
        _tool_name, _arg = _tool_name.strip(), _arg.strip()
        out = tool_dispatch(_tool_name, _arg)
        print(out)
        remember_entry("TOOL", user=user_prompt, tool=_tool_name, args=_arg, output=str(out)[:4000])
        return

    # ---------------------------------------------------------
    # 8) SZYBKIE INFO O KERNELU
    # ---------------------------------------------------------
    if prompt_lower in ["sprawd≈∫ kernel", "jaki kernel", "kernel", "wersja kernela"]:
        out = system_tool("uname -r")
        print(out)
        remember_entry("SYSTEM", user=user_prompt, command="uname -r", output=out[:4000])
        return

    # ---------------------------------------------------------
    # 9) INTENCJE LOKALNE (sieƒá, pamiƒôƒá, lista modeli, audio, cinnamon, logi itd.)
    # ---------------------------------------------------------
    # Uwaga: tool_name i arg ju≈º mamy z punktu (1). Nie wywo≈Çujemy detect_intent_local drugi raz,
    # ale NIC nie usuwam ‚Äî zostawiam TwojƒÖ logikƒô, tylko porzƒÖdkujƒô i stabilizujƒô.
    

    # ---------------------------------------------------------
    # 10) FILTR BEZPIECZE≈ÉSTWA DLA NIEZNANYCH / ZBYT D≈ÅUGICH PYTA≈É
    # ---------------------------------------------------------
    # Zostawiam Twoje komunikaty, ale porzƒÖdkujƒô indentacjƒô i usuwam ‚Äûmartwe‚Äù returny.
    if len(user_prompt) > 120:
        print(
            "‚ùó Nie rozumiem polecenia (liter√≥wka lub zbyt og√≥lne pytanie).\n"
            "üëâ Spr√≥buj kr√≥cej albo u≈ºyj jednego z polece≈Ñ:\n"
            "- przetestuj po≈ÇƒÖczenie z m√≥zgiem\n"
            "- zdiagnozuj d≈∫wiƒôk\n"
            "- sprawd≈∫ internet\n"
            "- sprawd≈∫ dyski\n"
            "- lista modeli\n"
            "- zmie≈Ñ model na <nazwa>\n"
        )
        return

    # ---------------------------------------------------------
    # 11) DODATKOWY TEST ‚ÄûCZY PO≈ÅƒÑCZENIE DZIA≈ÅA?‚Äù ‚Äì NIE MO≈ªE ZWRACAƒÜ RETURN KROTKI
    # ---------------------------------------------------------
    # U Ciebie by≈Ço: return "BRAIN_TEST", ""  -> to NIE MA SENSU w run_once() i psuje logikƒô.
    # Nic nie usuwam, tylko naprawiam wykonanie: kierujƒô to na tool_dispatch.
    if any(k in prompt_lower for k in [
        "czy po≈ÇƒÖczenie dzia≈Ça",
        "czy polaczenie dziala",
        "czy po laczenie dziala",
        "czy dzia≈Ça po≈ÇƒÖczenie",
        "czy dziala",
        "czy m√≥zg dzia≈Ça",
        "czy mozg dziala"
    ]):
        out = tool_dispatch("BRAIN_TEST", "")
        print(out)
        remember(user_prompt, out)
        return

    # ---------------------------------------------------------
    # 12) STATUS SYSTEMU (monitor/status)
    # ---------------------------------------------------------
    if "monitor" in prompt_lower or "status" in prompt_lower:
        s = get_status()
        report = analyze_status(s)
        print(report)
        remember(user_prompt, report)
        return

    # ---------------------------------------------------------
    # 13) SNAPSHOT STANU I AKTUALIZACJA
    # ---------------------------------------------------------
    update_state({
        "last_seen": datetime.now().isoformat(),
        "kernel": system_tool("uname -a", timeout=3)[:300],
        "session": system_tool("echo $XDG_SESSION_TYPE", timeout=3).strip(),
        "os": system_tool("cat /etc/os-release", timeout=3)[:500],
    })
    st = load_state()

    # ---------------------------------------------------------
    # 14) BANNER (informacyjny)
    # ---------------------------------------------------------
    print_lyra_banner(cfg)

    # ---------------------------------------------------------
    # 15) BUDOWA SYSTEM_MESSAGE Z KONTEKSTEM
    # ---------------------------------------------------------
    system_state_block = (
        f"last_seen: {st.get('last_seen','')}\n"
        f"kernel: {st.get('kernel','')}\n"
        f"os:\n{st.get('os','')}\n"
        f"last_tool: {st.get('last_tool','')} | {st.get('last_tool_arg','')}\n"
        f"last_tool_output:\n{st.get('last_tool_output','')}\n"
        f"last_system_cmd: {st.get('last_system_cmd','')}\n"
        f"last_system_output:\n{st.get('last_system_output','')}\n"
    )

    system_message = {
        "role": "system",
        "content": (
            "Jeste≈õ technicznƒÖ asystentkƒÖ Tomka w terminalu Linux. Masz na imiƒô LYRA. "
            "U≈ºytkownik ma na imiƒô Tomek. Odpowiadaj po polsku, konkretnie.\n"
            "Tryb: agresywny (3) ‚Äî dzia≈Çaj, nie filozofuj.\n\n"
            "Masz TRZY typy odpowiedzi:\n\n"
            "1) Zwyk≈Ça odpowied≈∫ tekstowa po polsku ‚Äì wyja≈õnienia, analizy, podsumowania.\n\n"
            "2) Odpowied≈∫ narzƒôdziowa:\n"
            "   - SYSTEM: <komenda bash>\n"
            "   - TOOL: <NAZWA_NARZƒòDZIA> | <argument>\n\n"
            "3) PROPOSE_TOOL:\n"
            "   NAME:\n"
            "   DESCRIPTION:\n"
            "   CODE:\n\n"
            "ZASADY:\n"
            "- SYSTEM / TOOL ‚Üí ZERO dodatkowego tekstu\n"
            "- ryzykowne akcje tylko jako PROPOSE_TOOL\n"
            "- *_FIX i AUTO_OPTIMIZE tylko je≈õli sƒÖ odwracalne\n\n"
            "- Nie zgaduj imion\n"
            "- Korzystaj z pamiƒôci rozmowy\n"
            "- Nie zmieniaj swojej to≈ºsamo≈õci\n\n"
            "ZASADA AUTOMATYCZNEJ DECYZJI:\n"
            "- Jesli polecenie dotyczy diagnozy, uzyj *_DIAG.\n"
            "- Jesli dotyczy naprawy, uzyj *_FIX.\n"
            "- Jesli dotyczy informacji, uzyj *_INFO.\n"
            "- Jesli dotyczy uruchomienia programu lub strony, uzyj: TOOL: APP_CONTROL | <opis>.\n"
            "- Jesli dotyczy pilnowania aplikacji, uzyj: TOOL: APP_GUARD | <opis>.\n"
            "- Jesli dotyczy plikow/katalogow/archiwow (find/cp/mv/grep/tar/unzip), uzyj: SYSTEM: <komenda>.\n"
            "- Jesli mozna wykonac od razu, NIE opisuj - zwroc tylko SYSTEM: albo TOOL:.\n"
            "- Jesli nie jest oczywiste, zapytaj krotko o doprecyzowanie.\n"
            "\n\n=== SYSTEM STATE (DO ZAPAMIƒòTANIA) ===\n"
            f"session: {st.get('session','')}\n"
            f"kernel: {st.get('kernel','')}\n"
            f"os:\n{st.get('os','')}\n"
            f"last_tool: {st.get('last_tool','')} {st.get('last_tool_arg','')}\n"
            f"last_tool_output:\n{st.get('last_tool_output','')}\n"
            f"last_system_cmd: {st.get('last_system_cmd','')}\n"
            f"last_system_output:\n{st.get('last_system_output','')}\n"
            f"=== SYSTEM_STATE ===\n{system_state_block}\n=== END STATE ===\n"
        )
    }

    # ---------------------------------------------------------
    # 16) KONTEKST Z PAMIƒòCI (ostatnie wpisy TEXT)
    # ---------------------------------------------------------
    mem = load_memory()[-6:]
    context_lines = []
    for m in mem:
        if m.get("type") == "TEXT":
            context_lines.append(f"User: {m.get('user','')}")
            context_lines.append(f"Lyra: {m.get('assistant','')}")
    context = "\n".join(context_lines)

    prompt = (
        f"Jeste≈õ LYRA.\nRozmawiasz z Tomkiem.\nZnasz jego imiƒô: Tomek.\n\n"
        f"KONTEKST ROZMOWY:\n{context}\n\n"
        f"AKTUALNE PYTANIE:\n{user_prompt}"
    )

    messages = [system_message, {"role": "user", "content": prompt}]

    # ---------------------------------------------------------
    # 17) WYB√ìR BACKENDU I WYWO≈ÅANIE MODELU
    # ---------------------------------------------------------
    backend = cfg.get("backend", "local")
    msg = ""
    mode = ""

    if backend == "local":
        try:
            ollama_model = cfg.get("ollama_model") or cfg.get("model") or "mistral"
            r = requests.post(
                "http://localhost:11434/api/generate",
                json={"model": ollama_model, "prompt": prompt},
                timeout=60,
                stream=True
            )
            for line in r.iter_lines():
                if line:
                    chunk = json.loads(line.decode("utf-8"))
                    msg += chunk.get("response", "")
            mode = f"local:{ollama_model}"
        except Exception as e:
            msg = f"[B≈ÇƒÖd lokalnego modelu] {e}"
            mode = "local:error"
    else:
        if not client:
            msg = "[B≈ÇƒÖd] Brak api_key w config.json."
            mode = "openai:error"
        else:
            try:
                openai_model = pick_openai_model(cfg)
                msg, mode = query_model(
                    user_prompt,
                    openai_model,
                    openai_model,
                    client,
                    messages
                )
            except Exception as e:
                msg = f"[B≈ÇƒÖd OpenAI] {e}"
                mode = "openai:error"

    msg = (msg or "").strip()

    # ---------------------------------------------------------
    # 18) FALLBACK DO ‚ÄûM√ìZGU‚Äù GDY LLM PAD≈Å / TIMEOUT / B≈ÅƒÑD
    # ---------------------------------------------------------
    if mode.endswith(":error") or msg.startswith("[B≈ÇƒÖd lokalnego modelu]"):
        log("Local model error ‚Üí fallback to brain", "agent.log")
        fallback = query_brain(polish_guard(prompt))


        # Je≈õli model lokalny nie jest Mistral, zaktualizuj komunikat w fallbacku
        model_name = cfg.get("local_model", "Mistral")
        if model_name and model_name.lower() != "mistral" and "Mistral" in fallback:
            fallback = fallback.replace("Mistral", model_name)

        print(fallback)
        remember(user_prompt, fallback)
        return

    # ---------------------------------------------------------
    # 19) FALLBACK DO M√ìZGU, GDY MODEL ZWR√ìCI≈Å PUSTKƒò
    # ---------------------------------------------------------
    if not msg:
        log("LLM EMPTY ‚Üí fallback to brain", "agent.log")
        out = query_brain(prompt)
        print(out)
        remember(user_prompt, out)
        return

    # ---------------------------------------------------------
    # 20) OBS≈ÅUGA ODPOWIEDZI: SYSTEM / TOOL / PROPOSE_TOOL / TEKST
    # ---------------------------------------------------------
    if msg.startswith("SYSTEM:"):
        cmd = msg[len("SYSTEM:"):].strip()
        out = system_tool(cmd)
        print(out)
        remember_entry("SYSTEM", user=user_prompt, command=cmd, output=str(out)[:4000])
        update_state({"last_system_cmd": cmd, "last_system_output": str(out)[:4000]})
        return

    if msg.startswith("TOOL:"):
        rest = msg[len("TOOL:"):].strip()
        _tool_name, _arg = (rest.split("|", 1) + [""])[:2]
        _tool_name, _arg = _tool_name.strip(), _arg.strip()
        out = tool_dispatch(_tool_name, _arg)
        print(out)
        remember_entry("TOOL", user=user_prompt, tool=_tool_name, args=_arg, output=str(out)[:4000])
        update_state({
            "last_tool": _tool_name,
            "last_tool_arg": _arg,
            "last_tool_output": str(out)[:4000]
        })
        return

    if msg.startswith("PROPOSE_TOOL:"):
        print(msg)
        remember_entry("PROPOSE_TOOL", user=user_prompt, proposal=msg[:4000])
        return

    # ---------------------------------------------------------
    # 21) NORMALNY TEKST
    # ---------------------------------------------------------
    print(msg)
    remember(user_prompt, msg)
    log(f"MODEL USED: {mode}", "agent.log")


# ======= PUNKT WEJ≈öCIA =======
if __name__ == "__main__":
    if len(sys.argv) > 1:
        run_once(" ".join(sys.argv[1:]))
    else:
        print("Lyra online. 'exit' aby wyj≈õƒá.")
        while True:
            try:
                user_input = input("Ty > ").strip()
            except (EOFError, KeyboardInterrupt):
                print()
                break
            if not user_input:
                continue
            if user_input.lower() in ("exit", "quit"):
                break
            run_once(user_input)

           
